{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0852934b-ed02-41fb-960f-48140a3bb62a",
   "metadata": {},
   "source": [
    "- Use ours Crop Data\n",
    "    - use Yolo/OpenCV/…… to detect people\n",
    "    - use post-processing to keep crop area w/h > 0.6\n",
    "    - Modify the transformation code (class CenterCrop(RandomCrop)) in the official mmaction\n",
    "        - 'mmaction2/mmaction/datasets/transforms/processing.py'\n",
    "        - origin official class have annotated\n",
    "        \n",
    "- The following is a simple sample code, and our cropped data is in the attachment.\n",
    "\n",
    "- **if you don't crop it (bellow pipeline), the effect is comparable**\n",
    "    | Model | Test F1_mean (TrainVal) |Test F1_mean (Train) | Val F1_mean | Crop |\n",
    "    | :-: | :-: | :-: | :-: | :-: | \n",
    "    | VideoMAE-Base-K710-Frame-Assisting-I3D-Head |-| 71.11 | 71.87 | No |\n",
    "    | VideoMAE-Base-K710-Frame-Assisting-I3D-Head |72.81| 71.14 | 72.33 | Yes|\n",
    "    \n",
    "```python\n",
    "train_pipeline = [\n",
    "    dict(type='DecordInit', **file_client_args),\n",
    "    dict(type='UniformSample', clip_len=num_frames, num_clips=1),\n",
    "    # dict(type='SampleFrames', clip_len=16, frame_interval=4, num_clips=1),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, 256)),\n",
    "    dict(type='RandomResizedCrop', area_range=(0.3, 1.0)),\n",
    "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
    "    dict(type='ColorJitter'),\n",
    "    dict(type='RandomErasing', max_area_ratio=0.2),\n",
    "    dict(type='Flip', flip_ratio=0.5),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='PackActionInputs')\n",
    "]\n",
    "val_pipeline = [\n",
    "    dict(type='DecordInit', **file_client_args),\n",
    "    dict(type='UniformSample', clip_len=num_frames, num_clips=2, test_mode=True),\n",
    "    # dict(type='SampleFrames', clip_len=16, frame_interval=4, num_clips=2, test_mode=True),\n",
    "    dict(type='DecordDecode'),\n",
    "    dict(type='Resize', scale=(-1, 256)),\n",
    "    dict(type='CenterCrop', crop_size=224),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='PackActionInputs')\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type='DecordInit', **file_client_args),\n",
    "    dict(type='UniformSample', clip_len=num_frames, num_clips=2, test_mode=True),\n",
    "    # dict(type='SampleFrames', clip_len=16, frame_interval=4, num_clips=2, test_mode=True),\n",
    "    dict(type='DecordDecode'),\n",
    "    # dict(type='Resize', scale=(-1, 256)),\n",
    "    # dict(type='CenterCrop', crop_size=224),\n",
    "    dict(type='Resize', scale=(-1, 224)),\n",
    "    dict(type='ThreeCrop', crop_size=224),\n",
    "    dict(type='FormatShape', input_format='NCTHW'),\n",
    "    dict(type='PackActionInputs')\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484311a9-8886-4bbf-ac0c-eed7f3f52095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def find_red_object(frame):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # lower_red1 = np.array([0, 70, 50])\n",
    "    # upper_red1 = np.array([10, 255, 255])\n",
    "    # lower_red2 = np.array([170, 70, 50])\n",
    "    # upper_red2 = np.array([180, 255, 255])\n",
    "    lower_red1 = np.array([0, 120, 50]) \n",
    "    upper_red1 = np.array([6, 255, 255]) \n",
    "    lower_red2 = np.array([174, 120, 50])  \n",
    "    upper_red2 = np.array([180, 255, 255]) \n",
    "    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask = mask1 + mask2\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=3)\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "def determine_crop_region(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    min_x, min_y = np.inf, np.inf\n",
    "    max_x, max_y = -np.inf, -np.inf\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        shape = frame.shape\n",
    "        contours = find_red_object(frame)\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            min_x = min(min_x, x)\n",
    "            min_y = min(min_y, y)\n",
    "            max_x = max(max_x, x + w)\n",
    "            max_y = max(max_y, y + h)\n",
    "    cap.release()\n",
    "    min_x = max(0, min_x - 60)\n",
    "    min_y = max(0, min_y - 100)\n",
    "    max_x = min(shape[1] - 1, max_x + 80)\n",
    "    max_y = min(shape[0] - 1, max_y + 60)\n",
    "    w = max_x - min_x\n",
    "    h = max_y - min_y\n",
    "    ratio = w / h\n",
    "    if ratio < 0.6:\n",
    "        padding = int((h * 0.6 - w) / 2)\n",
    "        min_x = max(0, min_x - padding)\n",
    "        max_x = min(shape[1] - 1, max_x + padding)\n",
    "        w = max_x - min_x\n",
    "    \n",
    "    # print(ratio, w/h)\n",
    "    return min_x, min_y, w, h\n",
    "\n",
    "def crop_video_to_region(video_path, output_path, crop_region):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video: {video_path}\")\n",
    "        return False\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (crop_region[2], crop_region[3]))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cropped_frame = frame[crop_region[1]:crop_region[1]+crop_region[3], crop_region[0]:crop_region[0]+crop_region[2]]\n",
    "        # print(cropped_frame.shape, cropped_frame.shape[1]/cropped_frame.shape[0])\n",
    "        out.write(cropped_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    # Verify the output video\n",
    "    if os.path.exists(output_path):\n",
    "        size = os.path.getsize(output_path)\n",
    "        if size < 1000:  # Arbitrarily chosen minimum file size (in bytes)\n",
    "            print(f\"Warning: The output file {output_path} is unusually small ({size} bytes)\")\n",
    "            return False\n",
    "        cap_check = cv2.VideoCapture(output_path)\n",
    "        if not cap_check.isOpened() or cap_check.get(cv2.CAP_PROP_FRAME_COUNT) < frame_count * 0.9:  # Check if 90% of frames are present\n",
    "            print(f\"Error: Output video {output_path} is incomplete or corrupted.\")\n",
    "            cap_check.release()\n",
    "            return False\n",
    "        cap_check.release()\n",
    "    return True\n",
    "\n",
    "def process_video_file(video_file):\n",
    "    video_path = os.path.join(input_dir, video_file)\n",
    "    output_path = os.path.join(output_dir, video_file)\n",
    "    crop_region = determine_crop_region(video_path)\n",
    "    if not crop_video_to_region(video_path, output_path, crop_region):\n",
    "        raise Exception(\"Crop video to region failed.\")\n",
    "        return f\"Failed {video_file}\"\n",
    "    return [crop_region[2], crop_region[3]]\n",
    "\n",
    "input_dir = 'autodl-tmp/train'\n",
    "output_dir = 'autodl-tmp/all_clip'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "video_files = [f for f in os.listdir(input_dir) if f.endswith('.mp4')]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "    results = list(tqdm(executor.map(process_video_file, video_files), total=len(video_files)))\n",
    "\n",
    "results = np.array(results)\n",
    "print(results.mean(0))\n",
    "print(\"All videos have been processed and saved to\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
